"""
Observability middleware providing structured logging, metrics, and error tracking.

Features:
- Structured JSON logging with request context via structlog
- Prometheus metrics for request latency, error rates, active requests
- Sentry integration for error tracking with stack traces
- Request timing with performance insights
"""

import time
from collections.abc import Callable
from contextvars import ContextVar
from typing import Any

import structlog
from fastapi import FastAPI, Request, Response
from prometheus_client import Counter, Gauge, Histogram, generate_latest
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.responses import Response as StarletteResponse

from app.config import get_settings

# Context variable for request-scoped logging context
request_context: ContextVar[dict[str, Any]] = ContextVar("request_context", default={})

# ============================================================================
# Prometheus Metrics
# ============================================================================

# Request metrics
REQUEST_COUNT = Counter(
    "http_requests_total",
    "Total HTTP requests",
    ["method", "endpoint", "status_code"],
)

REQUEST_LATENCY = Histogram(
    "http_request_duration_seconds",
    "HTTP request latency in seconds",
    ["method", "endpoint"],
    buckets=(0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0, 60.0),
)

REQUEST_IN_PROGRESS = Gauge(
    "http_requests_in_progress",
    "Number of HTTP requests in progress",
    ["method", "endpoint"],
)

# LLM-specific metrics
LLM_REQUEST_COUNT = Counter(
    "llm_requests_total",
    "Total LLM requests",
    ["model", "status"],
)

LLM_REQUEST_LATENCY = Histogram(
    "llm_request_duration_seconds",
    "LLM request latency in seconds",
    ["model"],
    buckets=(0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 30.0, 60.0, 120.0),
)

LLM_TOKENS_GENERATED = Counter(
    "llm_tokens_generated_total",
    "Total tokens generated by LLM",
    ["model"],
)

# Error metrics
ERROR_COUNT = Counter(
    "errors_total",
    "Total errors",
    ["type", "endpoint"],
)


# ============================================================================
# Structured Logging Setup
# ============================================================================


def configure_structlog(json_format: bool = True) -> None:
    """
    Configure structlog for structured logging.

    Args:
        json_format: If True, output JSON logs. Otherwise, console-friendly format.
    """
    processors = [
        structlog.contextvars.merge_contextvars,
        structlog.processors.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.stdlib.ExtraAdder(),
    ]

    if json_format:
        processors.extend([
            structlog.processors.format_exc_info,
            structlog.processors.JSONRenderer(),
        ])
    else:
        processors.extend([
            structlog.dev.ConsoleRenderer(colors=True),
        ])

    structlog.configure(
        processors=processors,
        wrapper_class=structlog.make_filtering_bound_logger(0),
        context_class=dict,
        logger_factory=structlog.PrintLoggerFactory(),
        cache_logger_on_first_use=True,
    )


def get_logger(name: str | None = None) -> structlog.BoundLogger:
    """
    Get a structured logger instance.

    Args:
        name: Logger name (typically __name__)

    Returns:
        Configured structlog bound logger
    """
    return structlog.get_logger(name)


# ============================================================================
# Sentry Integration
# ============================================================================


def init_sentry(
    dsn: str | None,
    environment: str = "development",
    traces_sample_rate: float = 0.1,
) -> None:
    """
    Initialize Sentry error tracking.

    Args:
        dsn: Sentry DSN. If None, Sentry is disabled.
        environment: Deployment environment name.
        traces_sample_rate: Percentage of transactions to trace (0.0-1.0).
    """
    if not dsn:
        return

    try:
        import sentry_sdk
        from sentry_sdk.integrations.fastapi import FastApiIntegration
        from sentry_sdk.integrations.starlette import StarletteIntegration

        sentry_sdk.init(
            dsn=dsn,
            environment=environment,
            traces_sample_rate=traces_sample_rate,
            integrations=[
                FastApiIntegration(transaction_style="endpoint"),
                StarletteIntegration(transaction_style="endpoint"),
            ],
            # Don't send PII by default
            send_default_pii=False,
            # Attach request data
            attach_stacktrace=True,
            # Before send hook for filtering
            before_send=_sentry_before_send,
        )

        logger = get_logger(__name__)
        logger.info("Sentry initialized", environment=environment)

    except ImportError:
        logger = get_logger(__name__)
        logger.warning("sentry-sdk not installed, error tracking disabled")


def _sentry_before_send(event: dict, hint: dict) -> dict | None:
    """
    Filter Sentry events before sending.

    Removes sensitive data and filters noise.
    """
    # Remove sensitive headers
    if "request" in event and "headers" in event["request"]:
        sensitive_headers = {"authorization", "cookie", "x-api-key"}
        event["request"]["headers"] = {
            k: v
            for k, v in event["request"]["headers"].items()
            if k.lower() not in sensitive_headers
        }

    return event


def capture_exception(error: Exception, **extra: Any) -> None:
    """
    Capture an exception to Sentry with additional context.

    Args:
        error: The exception to capture.
        **extra: Additional context to attach.
    """
    try:
        import sentry_sdk

        with sentry_sdk.push_scope() as scope:
            for key, value in extra.items():
                scope.set_extra(key, value)
            sentry_sdk.capture_exception(error)
    except ImportError:
        pass


# ============================================================================
# Observability Middleware
# ============================================================================


class ObservabilityMiddleware(BaseHTTPMiddleware):
    """
    Middleware that provides comprehensive request observability.

    Features:
    - Request timing with Prometheus metrics
    - Structured logging with request context
    - Error tracking integration
    """

    # Endpoints to exclude from metrics (health checks, metrics endpoint itself)
    EXCLUDE_PATHS = {"/health", "/ready", "/metrics", "/docs", "/redoc", "/openapi.json"}

    async def dispatch(self, request: Request, call_next: Callable) -> Response:
        """Process request with observability instrumentation."""
        path = request.url.path
        method = request.method

        # Skip metrics for excluded paths
        if path in self.EXCLUDE_PATHS:
            return await call_next(request)

        # Normalize path for metrics (replace IDs with placeholders)
        endpoint = self._normalize_path(path)

        # Get request ID from previous middleware
        request_id = getattr(request.state, "request_id", "unknown")

        # Set up structured logging context
        structlog.contextvars.clear_contextvars()
        structlog.contextvars.bind_contextvars(
            request_id=request_id,
            method=method,
            path=path,
            endpoint=endpoint,
            client_ip=self._get_client_ip(request),
        )

        logger = get_logger(__name__)
        start_time = time.perf_counter()

        # Track in-progress requests
        REQUEST_IN_PROGRESS.labels(method=method, endpoint=endpoint).inc()

        try:
            # Process request
            response = await call_next(request)
            status_code = response.status_code

            # Calculate duration
            duration = time.perf_counter() - start_time

            # Record metrics
            REQUEST_COUNT.labels(
                method=method,
                endpoint=endpoint,
                status_code=status_code,
            ).inc()

            REQUEST_LATENCY.labels(
                method=method,
                endpoint=endpoint,
            ).observe(duration)

            # Log request completion
            log_level = "warning" if status_code >= 400 else "info"
            getattr(logger, log_level)(
                "Request completed",
                status_code=status_code,
                duration_ms=round(duration * 1000, 2),
            )

            # Add timing header
            response.headers["X-Response-Time"] = f"{duration * 1000:.2f}ms"

            return response

        except Exception as e:
            # Calculate duration even on error
            duration = time.perf_counter() - start_time

            # Record error metrics
            ERROR_COUNT.labels(
                type=type(e).__name__,
                endpoint=endpoint,
            ).inc()

            REQUEST_COUNT.labels(
                method=method,
                endpoint=endpoint,
                status_code=500,
            ).inc()

            # Log error
            logger.exception(
                "Request failed",
                error=str(e),
                error_type=type(e).__name__,
                duration_ms=round(duration * 1000, 2),
            )

            # Capture to Sentry
            capture_exception(e, request_id=request_id, path=path, method=method)

            raise

        finally:
            # Decrement in-progress counter
            REQUEST_IN_PROGRESS.labels(method=method, endpoint=endpoint).dec()

    def _normalize_path(self, path: str) -> str:
        """
        Normalize path for metrics by replacing dynamic segments.

        /api/v1/sessions/abc123 -> /api/v1/sessions/{id}
        """
        parts = path.split("/")
        normalized = []

        for i, part in enumerate(parts):
            if not part:
                continue

            # Check if this looks like an ID (UUID or alphanumeric)
            if len(part) > 8 and (
                self._is_uuid(part)
                or (part.isalnum() and not part.isalpha())
            ):
                normalized.append("{id}")
            else:
                normalized.append(part)

        return "/" + "/".join(normalized) if normalized else "/"

    def _is_uuid(self, value: str) -> bool:
        """Check if value looks like a UUID."""
        if len(value) == 36 and value.count("-") == 4:
            return True
        if len(value) == 32 and value.isalnum():
            return True
        return False

    def _get_client_ip(self, request: Request) -> str:
        """Extract client IP from request, handling proxies."""
        # Check forwarded headers
        forwarded = request.headers.get("x-forwarded-for")
        if forwarded:
            return forwarded.split(",")[0].strip()

        real_ip = request.headers.get("x-real-ip")
        if real_ip:
            return real_ip

        # Fall back to direct client
        if request.client:
            return request.client.host

        return "unknown"


# ============================================================================
# Metrics Endpoint
# ============================================================================


async def metrics_endpoint() -> StarletteResponse:
    """
    Prometheus metrics endpoint.

    Returns metrics in Prometheus text format.
    """
    return StarletteResponse(
        content=generate_latest(),
        media_type="text/plain; version=0.0.4; charset=utf-8",
    )


# ============================================================================
# LLM Metrics Helpers
# ============================================================================


class LLMMetrics:
    """Helper class for recording LLM-specific metrics."""

    @staticmethod
    def record_request(
        model: str,
        duration: float,
        tokens: int = 0,
        success: bool = True,
    ) -> None:
        """
        Record LLM request metrics.

        Args:
            model: Model name/identifier.
            duration: Request duration in seconds.
            tokens: Number of tokens generated.
            success: Whether the request succeeded.
        """
        status = "success" if success else "error"
        LLM_REQUEST_COUNT.labels(model=model, status=status).inc()
        LLM_REQUEST_LATENCY.labels(model=model).observe(duration)

        if tokens > 0:
            LLM_TOKENS_GENERATED.labels(model=model).inc(tokens)

    @staticmethod
    def record_tokens(model: str, count: int) -> None:
        """Record token generation."""
        LLM_TOKENS_GENERATED.labels(model=model).inc(count)


# ============================================================================
# Setup Function
# ============================================================================


def setup_observability(app: FastAPI) -> None:
    """
    Set up all observability features for the application.

    Args:
        app: FastAPI application instance.
    """
    settings = get_settings()

    # Configure structured logging
    if settings.enable_structured_logging:
        configure_structlog(json_format=not settings.debug)

    # Initialize Sentry
    init_sentry(
        dsn=settings.sentry_dsn,
        environment=settings.sentry_environment,
        traces_sample_rate=settings.sentry_traces_sample_rate,
    )

    # Add observability middleware
    app.add_middleware(ObservabilityMiddleware)

    # Add metrics endpoint
    if settings.enable_metrics:
        from fastapi import APIRouter
        from fastapi.responses import PlainTextResponse

        metrics_router = APIRouter(tags=["observability"])

        @metrics_router.get("/metrics", response_class=PlainTextResponse)
        async def get_metrics():
            """Prometheus metrics endpoint."""
            return StarletteResponse(
                content=generate_latest(),
                media_type="text/plain; version=0.0.4; charset=utf-8",
            )

        app.include_router(metrics_router)

    logger = get_logger(__name__)
    logger.info(
        "Observability configured",
        structured_logging=settings.enable_structured_logging,
        metrics_enabled=settings.enable_metrics,
        sentry_enabled=bool(settings.sentry_dsn),
    )
